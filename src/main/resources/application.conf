akka {
  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "DEBUG"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "DEBUG"

  # Filter of log events that is used by the LoggingAdapter before
  # publishing log events to the eventStream.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  event-handlers = ["akka.event.slf4j.Slf4jLogger"]

  log-config-on-start = off

  log-dead-letters = 10

  log-dead-letters-during-shutdown = off

  log-sent-messages = off

  log-received-messages = off

  http {
    server {
      parsing.max-content-length = 10 GB
    }
  }

  actor {
    provider = "akka.remote.RemoteActorRefProvider"
    warn-about-java-serializer-usage = false
  }

  //    remote {
  enabled-transports = ["akka.remote.netty.tcp", "akka.remote.netty.NettyRemoteTransport"]

  remote {
    maximum-payload-bytes = 30000000 bytes
    netty.tcp {
      message-frame-size = 30000000b
      send-buffer-size = 30000000b
      receive-buffer-size = 30000000b
      maximum-frame-size = 30000000b
    }
  }
}

arcite {
  home_name = "home"
  snapshot_name = "experiments_snapshot.json"
  file_service {
    snapshot_name = "file_service.json"
  }
  organization {
    name = "Idorsia Ltd."
    department = "Drug Discovery"
    package = "com.idorsia.research"
    experiment_types = [
      {name = "microarray", description = "Agilent microarray", package = "microarray"},
      {name = "NGS", description = "Next generation sequencing", package = "ngs"},
      {name = "Mock", description = "Any kind of tests", package = "mock"},
      {name = "nanostring", description = "Nanostring experiments", package = "nanostring"},
      {name = "bioinfo", description = "General bioinfo experiments", package = "bioinfo"},
      {name = "Catwalk", description = "Catwalk behavior experiments", package = "catwalk"}
    ]
  }
  api {
    version = 1
    specification = """arcite API (/api/v1/):
          |
          |
          |GET  /experiments ==>  return all experiments summary info or a few hundred if there are too many
          |
          |
          |GET  /experiments?page=XX&max=YY ==>  return experiments summary info from page XX with YY max returned
          |
          |
          |GET  /experiments?search=searchString&maxHits=number ==>  searching for the given string in the experiments and returning a maximum of maxHits results
          |
          |
          |POST /experiments {"search" : "search string", "maxHits": 10} ==>   return all experiments for the given search string, same as previous but with post
          |
          |
          |GET  /experiment/{uid} ==>  return one experiment with every information regarding its design
          |
          |
          |DELETE /experiment/{uid} ==>  delete the experiment
          |
          |
          |PUT /experiment/{uid}/description ==> {"description" : "...."}  ==>  change description
          |
          |
          |GET  /experiment/{uid}/transforms ==>  returns all the transforms for this experiment
          |
          |
          |GET  /experiment/{uid}/transform/{transfUID}/selectable ==>  returns all the selectable values inherited from the transform (in case they would be usefull for the next transform)
          |
          |
          |GET  /experiment/{uid}/tots ==>  returns all the tree of transforms for this experiment
          |
          |
          |POST  /experiment/{uid}/file_upload/meta ==>  upload a file to the meta information section (e.g. curl --form "fileupload=@file" http://server:port/experiment/{uid}/file_upload/meta
          |
          |
          |POST  /experiment/{uid}/file_upload/raw ==>  upload a file to the raw data section (e.g. curl --form "fileupload=@file" http://server:port/experiment/{uid}/file_upload/raw
          |
          |
          |POST  /experiment/{uid}/properties ==>  add properties to the experiment {"property_name" : "property_value"}
          |
          |
          |DELETE  /experiment/{uid}/properties ==>  delete properties to the experiment {["property_name1", "property2"]}
          |
          |
          |GET  /experiment/{uid}/files/raw ==>  returns list of raw files
          |
          |
          |GET  /experiment/{uid}/files/meta ==>  returns list of meta files
          |
          |
          |GET  /experiment/{uid}/files ==>  returns list of all raw files for this experiment
          |
          |
          |GET /experiment/${uid}/logs ==>  Return logs for given experiment (paging enabled, page=XX, max=XXX)
          |
          |
          |POST /experiment {"experiment" : "...."}  ==>  add a new experiment
          |
          |
          |POST /experiment/{exp_uid}/clone {"owner": {"organization": "new organization path, if not specified, will use the original", "name": "new name"}, "description": "...."}  ==>  clone an experiment
          |
          |
          |GET  /experiment/{uid}/published ==>  returns list of all published results (transform artifacts)
          |
          |
          |DELETE /experiment/{uid}/published/{uid} ==>  mark published as deleted
          |
          |
          |POST  /experiment/{uid}/publish {transform: String, description: String, artifacts: List[String]} ==>  publishes the artifacts of a transform, defines the default artifact if required
          |
          |
          |POST /design {"experiment": "uid", "design": {"description" : "desc", "sampleConditions" : [[{"name": "AA1", "description": "AA1", "category": "sampleID"}, {"name": "ACT-1234", "description": "ACT-1234", "category": "compound"}]..]}} ==>  add design to experiment
          |
          |
          |POST /raw_data/files {"experiment": "uid", "filesAndTarget" : ["rawfiles list"], "transferFiles": boolean } ==>  defines raw files for a experiment, files and target is a map, one original file to a target name for the file. If target name is omited, arcite will take the current file name.
          |
          |
          |POST /raw_data/folder {"experiment": "uid", "folder" : "folder", "regex": "regex", "withSubfolder": boolean, "transferFiles": boolean} ==>  defines raw data folder with regex to pick up files for a experiment
          |
          |
          |POST /raw_data/from_source {"experiment": "uid", "source" : "sourceName", filesAndFolders: ["files", "folders" (includes sublfolders)] "regex": "regex" (optional, default: .*)} ==>  defines raw data folder from mounted source
          |
          |
          |GET  /transform_definitions ==>   returns all possible transform definitions
          |
          |
          |GET  /transform_definitions?search=search_string ==>  returns all transform definitions based on search criteria
          |
          |
          |GET  /transform_definition/{uid} ==>   one specific transform
          |
          |
          |POST /run_transform/{"experiment": "uid", "transfDefUID": "uid", "parameters": Map[String,String]} ==>  run the specified transform on the given experiment with the given json parameters as parameter object
          |
          |
          |POST /run_transform/on_transform/{"experiment": "uid", "transfDefUID": "uid", "transformOrigin" :"uid", "parameters":  Map[String,String]} ==>  run the specified transform on the given experiment starting from another transform, the given json parameter can be added
          |
          |
          |POST /run_transform/on_raw_data/{"experiment": "uid", "transfDefUID": "uid", "transformOrigin" :"uid", "parameters":  Map[String,String]} ==>  run the specified transform on the given experiment starting with the default raw data (usually the first transform), the given json parameter can be added
          |
          |
          |POST /run_transform/on_transform_with_exclusions/{"experiment": "uid", "transfDefUID": "uid", "transformOrigin" :"uid", "excludes": [], "excludesRegex": [], "parameters":  Map[String,String]} ==>  run the specified transform on the given experiment starting from another transform, the given json parameter can be added
          |
          |
          |POST /run_transform/on_raw_data_with_exclusions/{"experiment": "uid", "transfDefUID": "uid", "transformOrigin" :"uid", "excludes": [], "excludesRegex": [], parameters":  Map[String,String]} ==>  run the specified transform on the given experiment starting with the default raw data (usually the first transform), the given json parameter can be added
          |
          |
          |GET /job_status/{uid} ==>  information about the job with the given uid
          |
          |
          |GET /all_jobs_status ==>  information about all current jobs (running, completed, in progress, ...)
          |
          |
          |GET /running_jobs_status ==>  information about the current running jobs (including information about the transform and its progress)
          |
          |
          |GET /all_last_updates ==>  Return "last update" on all experiments
          |
          |
          |GET /recent_logs ==>  Return most recent logs on all experiments
          |
          |
          |GET /all_transforms ==>  Return all transforms for all experiments
          |
          |
          |GET /data_sources ==>  Return all data sources (usually mounts of lab equipments)
          |
          |
          |GET /data_sources/sourceName ==>  Return folders and files at the top level of the given data source
          |
          |
          |POST /data_sources {"sourceName" : "sourceName", subFolder" : ["a", "b"]} ==>  Return folders and files at the top level of the given data source
          |
          |
          |GET /application_logs ==>  Returns most recent application logs
          |
          |
          |GET /tree_of_transforms ==>  Returns the list of possible tree of transforms
          |
          |
          |POST /tree_of_transforms {experiment: expuid, treeOfTransformUID: treeOfTransfUID, properties: Map[String, String],
          | startingTransform: originTransfUID, exclusions: Set[String]} ==>  Start a tree of transforms
          |
          |
          |GET /organization ==>  Returns information about the organization hosting this installation of Arcite. Includes the accepted experiments types.
          |"""
  }
}

transform_worker {
  akka {
    remote {
      maximum-payload-bytes = 30000000 bytes
      netty.tcp {
        message-frame-size = 30000000b
        send-buffer-size = 30000000b
        receive-buffer-size = 30000000b
        maximum-frame-size = 30000000b
      }
    }
  }
}

transform_cluster {
  akka {
    remote {
      maximum-payload-bytes = 30000000 bytes
      netty.tcp {
        message-frame-size = 30000000b
        send-buffer-size = 30000000b
        receive-buffer-size = 30000000b
        maximum-frame-size = 30000000b
      }
    }
  }
}

experiments-manager {
  akka {
    remote {
      maximum-payload-bytes = 30000000 bytes
      netty.tcp {
        message-frame-size = 30000000b
        send-buffer-size = 30000000b
        receive-buffer-size = 30000000b
        maximum-frame-size = 30000000b
      }
    }
  }
}
